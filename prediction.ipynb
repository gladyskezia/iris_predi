{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTkvvJkmqkE9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Sigmoid function\n",
        "# ---------------------------------------------------------\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Logistic Regression (NumPy Implementation)\n",
        "# ---------------------------------------------------------\n",
        "class LogisticRegressionScratch:\n",
        "    def __init__(self, learning_rate=0.1, max_iters=3000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iters = max_iters\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.max_iters):\n",
        "            linear = np.dot(X, self.weights) + self.bias\n",
        "            predictions = sigmoid(linear)\n",
        "\n",
        "            # Gradients\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (predictions - y))\n",
        "            db = (1 / n_samples) * np.sum(predictions - y)\n",
        "\n",
        "            # Update weights\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear = np.dot(X, self.weights) + self.bias\n",
        "        probs = sigmoid(linear)\n",
        "        return (probs >= 0.5).astype(int)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Manual Metrics (NumPy Only)\n",
        "# ---------------------------------------------------------\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    return np.mean(y_true == y_pred)\n",
        "\n",
        "\n",
        "def precision_score(y_true, y_pred):\n",
        "    true_positive = np.sum((y_pred == 1) & (y_true == 1))\n",
        "    predicted_positive = np.sum(y_pred == 1)\n",
        "    return true_positive / predicted_positive if predicted_positive > 0 else 0\n",
        "\n",
        "\n",
        "def recall_score(y_true, y_pred):\n",
        "    true_positive = np.sum((y_pred == 1) & (y_true == 1))\n",
        "    actual_positive = np.sum(y_true == 1)\n",
        "    return true_positive / actual_positive if actual_positive > 0 else 0\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. Generate Synthetic Binary Dataset (2D)\n",
        "# ---------------------------------------------------------\n",
        "X, y = make_classification(\n",
        "    n_samples=500,\n",
        "    n_features=2,          # requirement: 2D dataset\n",
        "    n_redundant=0,\n",
        "    n_clusters_per_class=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. Train Model\n",
        "# ---------------------------------------------------------\n",
        "model = LogisticRegressionScratch(learning_rate=0.1, max_iters=3000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 6. Evaluation (MANUAL METRICS)\n",
        "# ---------------------------------------------------------\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred)\n",
        "rec = recall_score(y_test, y_pred)\n",
        "\n",
        "print(\"Final Weights:\", model.weights)\n",
        "print(\"Final Bias:\", model.bias)\n",
        "print(\"Accuracy:\", acc)\n",
        "print(\"Precision:\", prec)\n",
        "print(\"Recall:\", rec)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 7. Decision Boundary Plot\n",
        "# ---------------------------------------------------------\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "\n",
        "xx, yy = np.meshgrid(\n",
        "    np.linspace(x_min, x_max, 200),\n",
        "    np.linspace(y_min, y_max, 200)\n",
        ")\n",
        "\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "Z = model.predict(grid)\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, levels=[-1, 0, 1], alpha=0.3)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=20)\n",
        "plt.title(\"Decision Boundary - Logistic Regression (From Scratch)\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.savefig(\"decision_boundary.png\")\n",
        "plt.close()"
      ]
    }
  ]
}